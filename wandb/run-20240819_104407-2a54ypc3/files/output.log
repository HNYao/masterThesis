2024-08-19 10:44:11,374 [Worker 0] Is distributed: False
2024-08-19 10:44:11,375 [Process: 0] Starting training
2024-08-19 10:44:11,375 [Process: 0] EPOCH 1:
2024-08-19 10:44:11,377 [Worker 0] Loss fn: focal - FocalLoss(), Rank 0 - True
2024-08-19 10:44:12,926 [Process: 0] Synchronize training processes
2024-08-19 10:44:12,926 [Process: 0] Evaluating...
Error executing job with overrides: []
[Worker 0] Is distributed: False
[Process: 0] Starting training
[Process: 0] EPOCH 1:
[Worker 0] Loss fn: focal - FocalLoss(), Rank 0 - True
['To Front of the normal monitor, there is a the normal keyboard', 'the blue phone is at the Back of the normal phone']
------list
['To Front of the normal monitor, there is a the normal keyboard', 'the blue phone is at the Back of the normal phone']
<class 'str'>
<class 'str'>
----x_geo shape: torch.Size([2, 16, 512])
----x_rgb shape: torch.Size([2, 16, 480, 640])
-----scene_pcs shape: torch.Size([2, 512, 3])
-----align shape: torch.Size([2, 16, 512])
------x shape: torch.Size([2, 35, 512])
------fusion x shape: torch.Size([2, 4, 512])
------target shape: torch.Size([2, 512, 4])
tensor([[[0.2028, 0.3592, 0.1945,  ..., 0.1251, 0.4740, 0.1963],
         [0.3724, 0.2667, 0.1582,  ..., 0.1394, 0.1753, 0.4111],
         [0.2220, 0.1966, 0.4491,  ..., 0.5495, 0.1753, 0.1963],
         [0.2028, 0.1774, 0.1982,  ..., 0.1860, 0.1753, 0.1963]],
        [[0.3022, 0.1658, 0.5318,  ..., 0.2739, 0.2172, 0.1463],
         [0.1963, 0.4420, 0.0963,  ..., 0.3728, 0.2172, 0.2151],
         [0.1502, 0.2115, 0.0963,  ..., 0.1767, 0.3485, 0.1463],
         [0.3513, 0.1806, 0.2755,  ..., 0.1767, 0.2172, 0.4923]]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
outputs shape: torch.Size([2, 4, 512])
mask shape: torch.Size([2, 512, 4])
["the orange lamp is on the normal eraser's Front side", "the normal printer is on the normal plant's Front Right side"]
------list
["the orange lamp is on the normal eraser's Front side", "the normal printer is on the normal plant's Front Right side"]
<class 'str'>
<class 'str'>
----x_geo shape: torch.Size([2, 16, 512])
----x_rgb shape: torch.Size([2, 16, 480, 640])
-----scene_pcs shape: torch.Size([2, 512, 3])
-----align shape: torch.Size([2, 16, 512])
------x shape: torch.Size([2, 35, 512])
------fusion x shape: torch.Size([2, 4, 512])
------target shape: torch.Size([2, 512, 4])
tensor([[[0.2753, 0.2787, 0.2372,  ..., 0.2807, 0.2840, 0.2561],
         [0.2949, 0.2752, 0.3040,  ..., 0.2880, 0.2994, 0.2955],
         [0.2096, 0.1945, 0.2294,  ..., 0.1882, 0.1995, 0.2223],
         [0.2201, 0.2516, 0.2294,  ..., 0.2430, 0.2170, 0.2261]],
        [[0.2921, 0.1181, 0.2656,  ..., 0.2035, 0.0474, 0.2759],
         [0.2489, 0.1181, 0.2659,  ..., 0.2624, 0.0474, 0.2761],
         [0.2006, 0.6458, 0.2082,  ..., 0.2035, 0.8577, 0.1848],
         [0.2583, 0.1181, 0.2602,  ..., 0.3306, 0.0474, 0.2631]]],
       device='cuda:0', grad_fn=<SoftmaxBackward0>)
outputs shape: torch.Size([2, 4, 512])
mask shape: torch.Size([2, 512, 4])
[Process: 0] Synchronize training processes
[Process: 0] Evaluating...
Traceback (most recent call last):
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/run.py", line 31, in main
    trainer.train()
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/trainer/trainer.py", line 356, in train
    self.model.inference_heatmap_4cls()
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/models/GeoL.py", line 135, in inference_heatmap_4cls
    colors = camp(normalized_class_1_feat.numpy())[:,:,3]
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.