2024-08-15 09:58:40,155 [Worker 0] Is distributed: False
2024-08-15 09:58:40,156 [Process: 0] Starting training
2024-08-15 09:58:40,156 [Process: 0] EPOCH 1:
2024-08-15 09:58:40,159 [Worker 0] Loss fn: soft_dice - SoftDiceLoss(), Rank 0 - True
[Worker 0] Is distributed: False
[Process: 0] Starting training
[Process: 0] EPOCH 1:
[Worker 0] Loss fn: soft_dice - SoftDiceLoss(), Rank 0 - True
['To Front of the normal monitor, there is a the normal keyboard', 'the blue phone is at the Back of the normal phone']
------list
['To Front of the normal monitor, there is a the normal keyboard', 'the blue phone is at the Back of the normal phone']
<class 'str'>
<class 'str'>
shape of recept: torch.Size([2, 3, 480, 640])
torch.float32
torch.float32
torch.float32
torch.float32
torch.float32
torch.float32
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/run.py", line 31, in main
    trainer.train()
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/trainer/trainer.py", line 339, in train
    avg_loss, avg_metrics = self.train_one_epoch(epoch)
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/trainer/trainer.py", line 201, in train_one_epoch
    loss = loss_fn(outputs, batch["mask"])
  File "/home/stud/zhoy/anaconda3/envs/o2o/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/trainer/losses.py", line 31, in forward
    union = inputs.sum(dims) + targets.sum(dims)
RuntimeError: The size of tensor a (2) must match the size of tensor b (640) at non-singleton dimension 1
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.