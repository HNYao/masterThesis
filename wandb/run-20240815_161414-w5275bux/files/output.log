2024-08-15 16:14:20,543 [Worker 0] Is distributed: False
2024-08-15 16:14:20,543 [Process: 0] Starting training
2024-08-15 16:14:20,544 [Process: 0] EPOCH 1:
2024-08-15 16:14:20,547 [Worker 0] Loss fn: soft_dice - SoftDiceLoss(), Rank 0 - True
[Worker 0] Is distributed: False
[Process: 0] Starting training
[Process: 0] EPOCH 1:
[Worker 0] Loss fn: soft_dice - SoftDiceLoss(), Rank 0 - True
['To Front of the normal monitor, there is a the normal keyboard', 'the blue phone is at the Back of the normal phone']
------list
['To Front of the normal monitor, there is a the normal keyboard', 'the blue phone is at the Back of the normal phone']
<class 'str'>
<class 'str'>
shape of recept: torch.Size([2, 3, 480, 640])
torch.float32
torch.float32
torch.float32
torch.float32
torch.float32
torch.float32
shape of inputs: torch.Size([2, 480, 640])
shape of targets: torch.Size([2, 480, 640])
["the orange lamp is on the normal eraser's Front side", "the normal printer is on the normal plant's Front Right side"]
------list
["the orange lamp is on the normal eraser's Front side", "the normal printer is on the normal plant's Front Right side"]
<class 'str'>
<class 'str'>
shape of recept: torch.Size([2, 3, 480, 640])
torch.float32
torch.float32
torch.float32
torch.float32
torch.float32
torch.float32
shape of inputs: torch.Size([2, 480, 640])
shape of targets: torch.Size([2, 480, 640])
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/run.py", line 31, in main
    trainer.train()
  File "/home/stud/zhoy/MasterThesis_zhoy/GeoL_net/trainer/trainer.py", line 339, in train
    avg_loss, avg_metrics = self.train_one_epoch(epoch)
TypeError: cannot unpack non-iterable float object
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.